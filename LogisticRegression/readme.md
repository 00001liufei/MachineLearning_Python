逻辑回归（logistics regression）
=========

[![MIT license](https://img.shields.io/dub/l/vibe-d.svg)](https://github.com/lawlite19/MachineLearning_Python/blob/master/LICENSE)

## 目录
* [一、分类和回归的区别](#一分类和回归的区别)
* [二、逻辑回归](#二逻辑回归)
	* [1、用回归方法进行分类](#1用回归方法进行分类)

## 一、分类和回归的区别

<br/>　　我们可以按照任务的种类,将任务分为回归任务和分类任务.那这两者的区别是什么呢?
按照较官方些的说法,输入变量与输出变量均为连续变量的预测问题是回归问题,输出变量为有限个离散变量的预测问题称为分类问题.
<br/>　　通俗一点讲,我们要预测的结果是一个数,比如要通过一个人的饮食预测一个人的体重,体重的值可以有无限多个,
有的人50kg,有的人51kg,在50和51之间也有无限多个数.这种预测结果是某一个确定数,而具体是哪个数有无限多种可能,
我们会训练出一个模型,传入参数后得到这个确定的数,这类问题我们称为回归问题。
预测的这个变量(体重)因为有无限多种可能,在数轴上是连续的,所以我们称这种变量为连续变量.
<br/>　　我们要预测一个人身体健康或者不健康,预测会得癌症或者不会得癌症,预测他是水瓶座,天蝎座还是射手座,
这种结果只有几个值或者多个值的问题,我们可以把每个值都当做一类,预测对象到底属于哪一类。
这样的问题称为分类问题。如果一个分类问题的结果只有两个,比如"是"和"不是"两个结果,
我们把结果为"是"的样例数据称为"正例",讲结果为"不是"的样例数据称为"负例",对应的,这种结果的变量称为离散型变量。

## 二、逻辑回归
<br/>　　从名字来理解逻辑回归.在逻辑回归中,逻辑一词是logistics的音译字,并不是因为这个算法是突出逻辑的特性.
<br/>　　至于回归,我们前一段讲到回归任务是结果为连续型变量的任务,logistics regression是用来做分类任务的,
为什么叫回归呢?那我们是不是可以假设,逻辑回归就是用回归的办法来做分类的呢.

### 1、用回归方法进行分类

<br/>　　假设刚刚的思路是正确的,逻辑回归就是在用回归的办法做分类任务,那有什么办法可以做到呢,
此时我们就先考虑最简单的二分类,结果是正例或者负例的任务.

<br/>　　按照多元线性回归的思路,我们可以先对这个任务进行线性回归,学习出这个事情结果的规律,
比如根据人的饮食,作息,工作和生存环境等条件预测一个人"有"或者"没有"得恶性肿瘤,可以先通过回归任务来预测人体内肿瘤的大小,
取一个平均值作为阈值,假如平均值为y,肿瘤大小超过y为恶性肿瘤,无肿瘤或大小小于y的,为非恶性.
这样通过线性回归加设定阈值的办法,就可以完成一个简单的二分类任务.如下图:

![enter description here][1]

<br/>　　上图中x轴为肿瘤大小，粉色线为回归得到的函数![{h_\theta}(x)](http://chart.apis.google.com/chart?cht=tx&chl=h_\theta}(x))
，绿色的线为阈值。

<br/>　　 预测肿瘤大小还是一个回归问题,得到的结果(肿瘤的大小)也是一个连续型变量.
通过设定阈值,就成功将回归问题转化为了分类问题.但是,这样做还存在一个问题.

<br/>　　 我们上面的假设,依赖于所有的肿瘤大小都不会特别离谱,如果有一个超大的肿瘤在我们的例子中,
阈值就很难设定.加入还是取平均大小为阈值,则会出现下图的情况:

![enter description here][2]

<br/>　　 从上边的例子可以看出,使用线性的函数来拟合规律后取阈值的办法是行不通的,
行不通的原因在于拟合的函数太直,离群值(也叫异常值)对结果的影响过大,
但是我们的整体思路是没有错的,错的是用了太"直"的拟合函数,如果我们用来拟合的函数是非线性的,不这么直,是不是就好一些呢?

<br/>　　 所以我们下面来做两件事:
- 找到一个办法解决掉回归的函数严重受离群值影响的办法.
- 选定一个阈值.

### 2、选择回归函数
　　 原来的判别函数是用线性的![y=w^{T}x](http://chart.apis.google.com/chart?cht=tx&chl=y=w^{T}x)，
逻辑回归目前用的是sigmoid函数，函数如下：

![g(z) = \frac{1}{{1 + {e^{ - z}}}}](http://chart.apis.google.com/chart?cht=tx&chl={g(z)=\frac{1}{{{1}%2B{e^{-z}}}}})

公式中，z是多元线性回归中的![w^{T}x](http://chart.apis.google.com/chart?cht=tx&chl=w^{T}x)

sigmod函数图像如下：
![enter description here][3]

　　 该函数具有很强的鲁棒性，并且将函数的输入范围(-∞,∞)映射到了输出的(0,1)之间且具有概率意义。具有概率意义是怎么理解呢:
将一个样本输入到我们学习到的函数中,输出0.7,意思就是这个样本有70%的概率是正例,1-70%就是30%的概率为负例。

　　 总结一下上边所讲:我们利用线性回归的办法来拟合然后设置阈值的办法容易受到离群值的影响,
sigmod函数可以有效的帮助我们解决这一个问题,所以我们只要在拟合的时候把![y=w_{0}x_{0}%2Bw_{1}x_{1}%2B\cdots%2Bw_{n}x_{n}](http://chart.apis.google.com/chart?cht=tx&chl={y=w_{0}x_{0}%2Bw_{1}x_{1}%2B\cdots%2Bw_{n}x_{n}})
代入sigmoid函数即可，公式如下：

![g(z) = \frac{1}{{1 + {e^{ - w^{T}x}}}}](http://chart.apis.google.com/chart?cht=tx&chl={g(z)=\frac{1}{{{1}%2B{e^{-w^{T}x}}}}})

　　 同时，由于g(x)的特性，它输出的结果不再是预测值，而是表示值预测为正例和负例的概率，表达式如下

![P(y=0|w,x)=1-g(z)](http://chart.apis.google.com/chart?cht=tx&chl={P(y=0|w,x)=1-g(z)})
<br/>![P(y=1|w,x)=g(z)](http://chart.apis.google.com/chart?cht=tx&chl={P(y=1|w,x)=g(z)})

　　 全部预测正确的概率
![P(y=1)=g(w,xi)^{y^{i}}*(1-g(w,xi)^{1-y^{i}}](http://chart.apis.google.com/chart?cht=tx&chl={P(y=1)=g(w,xi)^{y^{i}}*(1-g(w,xi)^{1-y^{i}})})

其中![y^{i}](http://chart.apis.google.com/chart?cht=tx&chl={y^{i}})为某一条样本的预测值,取值范围为0或者1.

　　 到这里,我们得到一个回归函数,它不再像y=wT * x一样受离群值影响,
他的输出结果是样本预测为正例的概率(0到1之间的小数).我们接下来解决第二个问题:选定一个阈值.


### 3、选定阈值










----------------------------------


  [1]: ../images/logisticsRegression_04.jpg "logisticsRegression_04.jpg"
  [2]: ../images/logisticsRegression_05.jpg "logisticsRegression_05.jpg"
  [3]: ../images/logisticsRegression_06.jpg "logisticsRegression_06.jpg"

















